\documentclass{article}

\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath} % useful
\usepackage{amsfonts} % fonts like \mathbb{R} among others
\usepackage{amssymb}
\usepackage{amsthm} % used for writing lemmas/theorems/etc
\usepackage{tikz} % for visual tools
\usepackage{authblk}
\usepackage{enumitem}
\numberwithin{equation}{section}
\usepackage[margin=1in]{geometry}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{remark}{Remark}[theorem]
\newtheorem{example}{Example}[theorem]

\title{
    MATH60130 - Stochastic Differential Equations in Financial Modelling}
\author{Lectured by Damiano Brigo\\
Spring 2024\\
Transcribed by a muppet from Sesame Street
}
\date{}

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Nat}{\mathbb{N}}
\newcommand{\pr}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\textup{Var}}
\newcommand{\VaR}{\textup{VaR}}
\newcommand{\dm}{\mathrm{d}}
\newcommand{\ind}{\mathbf{1}}
\newcommand{\cov}{\textup{Cov}}
\newcommand{\corr}{\textup{Corr}}

\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\pspace}{(\Omega, \mathcal{F}, \mathbb{P})}

\newcommand{\sm}{\setminus}

\newcommand{\ie}{\textit{i}.\textit{e}.}
\newcommand{\eg}{\textit{e}.\textit{g}.}

\newcommand{\dd}[2]{\frac{\mathrm{d} #1}{\mathrm{d} #2}}
\newcommand{\ddn}[3]{\frac{\mathrm{d}^{#3} #1}{\mathrm{d}^{#3} #2}}
\newcommand{\pp}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\set}[1]{\{#1\}}

\begin{document}

\maketitle


\section{Revision of Probability}
I'm sure you remember these things by now, but in case you forgot what probability theory is, here's a brief recap.
\begin{definition}[Probability Space]
    A probability space $\pspace$ consists of three objects.
    \begin{itemize}
        \item A sample space $\Omega$ which is the set of all elementary outcomes of a random experiment. For example, if we are rolling a 6-sided die, we could take $\Omega = \set{1,2,3,4,5,6}$. Or if we are measuring a mechanical object, we could take $\Omega = \Q$. Most of the time, we will take $\Omega = \R$, the real numbers.
        \item A $\sigma$-algebra $\mc{F}$, which is a subset of the power set of $\Omega$. It is the set of all possible subsets of $\Omega$ that we could be interested in, called events. The following properties hold:
        \begin{itemize}
            \item $\mc{F}$ is non-empty and contains $\Omega$: $\mc{F} \neq \emptyset$ and $\Omega \in \mc{F}$.
            \item $\mc{F}$ is closed under \textbf{complements}: $A \in \mc{F} \implies A^c \in \mc{F}$.
            \item $\mc{F}$ is closed under \textbf{countable unions}: if $A_n \in \mc{F}$ for $n \in \Nat$, then $\bigcup_{n \in \Nat} A_n \in \mc{F}$.
        \end{itemize}
        Examples include the trivial $\sigma$-algebra, or the power set algebra of $\Omega$. More commonly, we will use the Borel $\sigma$-algebra, which is the smallest possible $\sigma$-algebra that contains all \textbf{open sets} of $\R$. This algebra is very big already, and includes all intervals and basically any set we really want to work with. We use the notation $\sigma(A)$ to denote a $\sigma$-algebra generated by a set of subsets of $\Omega$: the smallest $\sigma$-algebra that contains $A$. As such, the Borel $\sigma$-algebra $\mc{B}(\R) = \sigma(O)$ under this notation.
        \item A probability measure $\pr:\mc{F} \to [0,1]$ which assigns a probability to every event. The probability measure must satisfy these properties.
        \begin{itemize}
            \item $\pr(\Omega) = 1$
            \item $\pr$ is \textbf{countably additive}: if $A_n \in \mc{F}, n \in \Nat$ are disjoint sets, then $\pr(\bigcup_{n \in \Nat} A_n) = \sum_{n \in \Nat} \pr(A_n)$.
        \end{itemize}
    \end{itemize}
\end{definition}
This definition can be found in any probability theory textbook, please see them for further details.
\begin{definition}[Random Variable]
    A random variable $X$ on $\pspace$ is a \textbf{measurable} function $X:\Omega \to \R$. This means that for any Borel set $B \subset \R$, then $X^{-1}(B) \in \mc{F}$, that is the preimage is an event.
\end{definition}
The basic idea is that with this condition, one can now take the probability of things like $X^{-1}(B)$: the event that $X$ lies in $B$.
\begin{definition}[CDF and PDF]
    The \textbf{cumulative distribution function} of $X$ is defined as 
    \begin{equation}
        F_x(b) = \pr(X^{-1}((-\infty, b]))
    \end{equation}
    From now on we use the shorthand $\pr(X \leq b)$. Actually, we rarely use the preimage notation for events and always shorten to things like this.
    If there exists a function $p_x: \R \to [0, \infty]$ such that
    \begin{equation}
        F_x(b) = \int_{-\infty}^b p_X(y) \dm y
    \end{equation}
    then $p_X$ is the \textbf{probability density function} of $X$. One can then see that if $F_X$ is differentiable, then $F_X' = p_X$. One then also notes that the integral of $p_X$ must be $1$ as well.
\end{definition}

\begin{example}[Common random variables]
    \begin{itemize}
        \item Uniform random variable: $X \sim U(a,b)$ has density $\frac{1}{b-a}$ in the interval $[a,b]$ and $0$ otherwise.
        \item Normal random variable: $X \sim N(\mu, \sigma^2)$ has density $\frac{1}{\sigma \sqrt{2\pi}} e^{-\frac 12 (\frac{x - \mu}{\sigma})^2}$. The meaning of $\mu$ and $\sigma^2$ will become clearer later on. We call a standard normal random variable the particular case where $\mu = 0$ and $\sigma=1$.
    \end{itemize}
\end{example}
\begin{definition}[Expectation and variance]
    The \textbf{expected value or expectation} of a random variable $X$ is given by
    \begin{equation}
        \mu_X = \E(X) = \int_{\Omega} X \dm \pr = \int_{\R} y \dm F_x(y) = \int_{\R} y p_X(y) \dm y
    \end{equation}
    The first integral is a Lebesgue integral (see measure theory), the second one is a Stieltjes integral and the third one is a regular real integral.
    The \textbf{variance} is defined as
    \begin{equation}
        \V(X) = \sigma_X^2 = \E((X-\mu_X)^2) = \E(X^2) - \E(X)^2
    \end{equation}
    The square root of variance is \textbf{standard deviation}, sometimes denoted as $\text{Std}(X)$.
\end{definition}
\begin{theorem}
    \begin{itemize}
        \item Expectation is linear: $\E(aX+Y) = a\E(X) + \E(y)$. This holds for any pair of random variables.
        \item $\V(aX) = a^2 \V(X)$. Note that variance and standard deviation are not linear anymore (at least, not all the time in the case of variance).
    \end{itemize}
\end{theorem}
One can perform a few integrals to see that the parameters in the normal distribution are precisely the mean and variance.
\begin{theorem}[Properties of the Normal]
    The mean of the normal is $\mu$. The standard deviation is $\sigma$. The skewness (asymmetry in tails), defined as $\frac{\E((X-\mu)^3)}{\sigma^3}$ is $0$. The excess kurtosis (fatness of tails), defined as $\frac{\E((X-\mu)^4)}{\sigma^4}$ is $3$.
\end{theorem}
Later on we will use these values to compare if a given distribution is fatter or thinner than a normal random variable, or as a basic test if a random variable is normal.

\begin{definition}[Moment generating function]
    The \textbf{moment generating function} of a random variable $X$ is defined as
    \begin{equation}
        M_X(t) = \E(e^{tX})
    \end{equation}
    For example, the moment generating function of the normal distribution is $e^{\mu t + \frac{\sigma^2 t^2}{2}}$.
\end{definition}
It is moment generating because the derivative evaluated at $0$ is the first moment $\E(X)$ and the second derivative evaluated at $0$ is the second moment $\E(X^2)$. This pattern continues onwards.
\begin{remark}
    Not all random variables have a moment generating function. However, if you search up \textit{characteristic function} you will find something similar, which always exists. They basically play the same role.
\end{remark}
\begin{definition}[Exponential random variable]
    A random variable is an \textbf{exponential} random variable if it has CDF
    \begin{equation}
        F_X(y) = (1 - e^{-\lambda y}) \ind_{[0, \infty)}
    \end{equation}
    In the above, we used indicator function notation, and $\lambda$ is a free parameter, normally known as the rate. It is always positive.

    The density is given by
    \begin{equation}
        p_X(y) = \lambda e^{-\lambda y} \ind_{[0, \infty)}
    \end{equation}
    The expectation is $\frac{1}{\lambda}$ and the variance is $\frac{1}{\lambda^2}$.
    \textbf{Lack of memory property}: $\pr(X > x+y | X>y) = \pr(X>x)$, where we used a conditional probability.
\end{definition}
In fact, the exponential distribution is basically unique if only this property is assumed. We often use this random variable to model things like arrival times. In finance, it is used to model default times.

\begin{definition}[Lognormal random variable]
    A random variable $Y$ is lognormal if $Y = e^X$ where $X$ is normal. Thus the expectation is done using the MGF and results in
    \begin{equation}
        \E(Y) = \E(e^X) = M_X(1) = e^{\mu + \frac{1}{2}\sigma^2}
    \end{equation}
    Similarly, the variance is given by
    \begin{equation}
        \V(Y) = \V(e^X) = M_X(2) - M_X(1)^2 = e^{2\mu + 2\sigma^2} - e^{2\mu + \sigma^2}
    \end{equation}
\end{definition}
This random variable will pop up a lot in our study of SDEs, particularly in the case of geometric brownian motion.

\begin{definition}[Independence]
    Two random variables $X$ and $Y$ are said to be \textbf{independent} if $\pr(X \in A \cup Y \in B) = \pr(X \in A)\pr(Y \in B)$.
    This is equivalent to the joint CDF $F_{XY}$ factoring out into $F_X F_Y$.
\end{definition}
\begin{theorem}
    If $X$ and $Y$ are independent, then
    \begin{itemize}
        \item $\E(XY) = \E(X) \E(Y)$
        \item $\V(X+Y) = \V(X) + \V(y)$
        \item If $X \sim N(\mu_1, \sigma_1^2)$ and $Y \sim N(\mu_2, \sigma_2^2)$, then
        \begin{equation}
            X+Y \sim N(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)
        \end{equation}
        \item If $X$ and $Y$ are lognormal, then their product is lognormal as well. The specifics of the distribution can be derived from the above statement about normal distributions.
    \end{itemize}
\end{theorem}
\begin{theorem}[Central limit theorem]
    Suppose $X_n$ are a sequence of independent and identically distributed random variables, with finite mean and variance. Then we have
    \begin{equation}
        \sqrt(n) \frac{\bar{X}_n - \mu}{ \sigma} \to^D N(0,1)
    \end{equation}
    The above arrow means convergence in distribution: the CDF converges pointwise to the CDF of a standard normale everywhere (Note that it is really everywhere since the limit is continuous, but in general pointwise convergence is only needed for all continuity points of the limiting CDF).
\end{theorem}
The above theorem is why normal random variables are so powerful: they appear quite naturally, even if we don't know much about $X_n$ itself.

\begin{definition}[Multivariate random variables]
    Same thing as regular random variables, except they map into $\R^n$ and we consider the Borel $\sigma$-algebra of $\R^n$ instead. We may write $X$ as a vector $(X_1, X_2, \dots, X_n)$ if we so wish.

    The CDF is given by 
    \begin{equation}
        F_X(x_1, x_2, \dots, x_n) = \pr(X_1 \leq x-1 \cup X_2 \leq x_2 \cup \dots \cup X_n \leq x_n)
    \end{equation}
    and if there exists a function $p_X$ from $R^n$ to the positive reals such that
    \begin{equation}
        F_X(x_1, x_2, \dots, x_n) = \int_{-\infty}^{x_1} \int_{-\infty}^{x_2} \dots \int_{-\infty}^{x_n} p_X(y_1, y_2, \dots y_n) \dm y_1 \dm y_2 \dots \dm y_n
    \end{equation}
    Then $p_X$ is the PDF of $X$.
    Lastly, if $F_X$ is differentiable $n$ times, then $\frac{\partial^n F_X}{\partial x_1 \dots \partial x_n} = p_X$.
\end{definition}
\begin{definition}[Covariance and correlation]
    Suppose we have two random variables $X$ and $Y$, not necessarily independent. The \textbf{covariance} of $X$ and $Y$ is defined as
    \begin{equation}
        \cov(X,Y) = \E((X-\mu_X)(Y - \mu_Y)) = \E(XY) - \E(X)\E(Y)
    \end{equation}
    Note that the covariance of a random variable with itself is the variance. The covariance tells us how much $X$ and $Y$ are tied together, how much do they vary together. If $X$ and $Y$ are independent, then their covariance is $0$. The converse is not true, in which case we call the random variables uncorrelated. However, if two normal random variables are uncorrelated then they are independent (special case).

    Correlation is defined as

    \begin{equation}
        \corr(X,Y) = \rho(X,Y) = \frac{\cov(X,Y)}{\sigma_X \sigma_Y} \in [-1,1]
    \end{equation}
    The last statement is a fact, due to the Cauchy-Schwartz inequality for Hilbert spaces.
\end{definition}
It is important to know that correlation expresses the \textit{linear} dependence between two random variables. Some random variables could be coupled together nonlinearly \eg $X$ and $X^2$, and have correlation less that $1$, despite one being totally dependent on the other.
\end{document}